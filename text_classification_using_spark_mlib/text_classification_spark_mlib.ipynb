{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "https://www.kaggle.com/datasets/praveengovi/emotions-dataset-for-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # install dependencies\n",
    "# !pip install pyspark\n",
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# set panas to print full text\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load()\n",
    "\n",
    "# hide pyspark warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data which is in txt format\n",
    "df = pd.read_csv(\"train.txt\", names=[\"text\", \"emotion\"], delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's save the data in csv format with header\n",
    "df.to_csv(\"train.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                           text  \\\n",
       "0                                                                                       i didnt feel humiliated   \n",
       "1  i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake   \n",
       "\n",
       "   emotion  \n",
       "0  sadness  \n",
       "1  sadness  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the first 2 rows\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the required libraries for creating spark session\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.115:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb9934eadf0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see the existing spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the existing spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local appName=pyspark-shell>\n"
     ]
    }
   ],
   "source": [
    "# Create a SparkContext instance\n",
    "spark_context = SparkContext(master=\"local\")\n",
    "\n",
    "# Creating a spark session.\n",
    "spark = SparkSession.builder.appName(\"Emotion Detection\").getOrCreate()\n",
    "print(spark_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.115:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=pyspark-shell>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- emotion: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the train.csv file\n",
    "df = spark.read.csv(\"train.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# print the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|                text|emotion|\n",
      "+--------------------+-------+\n",
      "|i didnt feel humi...|sadness|\n",
      "|i can go from fee...|sadness|\n",
      "+--------------------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display the first 2 rows\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text       0\n",
       "emotion    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show how many null values are there in each column using pandas\n",
    "df.toPandas().isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "| emotion|count|\n",
      "+--------+-----+\n",
      "|     joy| 5362|\n",
      "|    love| 1304|\n",
      "|   anger| 2159|\n",
      "|    fear| 1937|\n",
      "|surprise|  572|\n",
      "| sadness| 4666|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display the data distribution of each emotion\n",
    "df.groupBy(\"emotion\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (\n",
    "    Tokenizer,\n",
    "    StopWordsRemover,\n",
    "    CountVectorizer,\n",
    "    IDF,\n",
    "    StringIndexer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text column\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|                text|emotion|               words|            filtered|        raw_features|            features|label|\n",
      "+--------------------+-------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|i didnt feel humi...|sadness|[i, didnt, feel, ...|[didnt, feel, hum...|(15082,[0,48,567]...|(15082,[0,48,567]...|  1.0|\n",
      "|i can go from fee...|sadness|[i, can, go, from...|[go, feeling, hop...|(15082,[1,29,42,5...|(15082,[1,29,42,5...|  1.0|\n",
      "+--------------------+-------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# remove the stop words\n",
    "stopwords_remover = StopWordsRemover(\n",
    "    inputCol=tokenizer.getOutputCol(), outputCol=\"filtered\"\n",
    ")\n",
    "\n",
    "# convert the words to vectors\n",
    "count_vectorizer = CountVectorizer(\n",
    "    inputCol=stopwords_remover.getOutputCol(), outputCol=\"raw_features\"\n",
    ")\n",
    "\n",
    "# calculate the idf\n",
    "idf = IDF(inputCol=count_vectorizer.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "\n",
    "# convert the emotion column to label\n",
    "label_stringIdx = StringIndexer(inputCol=\"emotion\", outputCol=\"label\")\n",
    "\n",
    "# import the required libraries for creating the pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# create the pipeline\n",
    "pipeline = Pipeline(\n",
    "    stages=[tokenizer, stopwords_remover, count_vectorizer, idf, label_stringIdx]\n",
    ")\n",
    "\n",
    "# fit the pipeline to the data\n",
    "pipeline_fit = pipeline.fit(df)\n",
    "\n",
    "# transform the data\n",
    "dataset = pipeline_fit.transform(df)\n",
    "\n",
    "# display the first 2 rows\n",
    "dataset.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "| emotion|label|\n",
      "+--------+-----+\n",
      "|    love|  4.0|\n",
      "|     joy|  0.0|\n",
      "|surprise|  5.0|\n",
      "|   anger|  2.0|\n",
      "| sadness|  1.0|\n",
      "|    fear|  3.0|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show which joy belongs to which encoded label\n",
    "dataset.select(\"emotion\", \"label\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 11246\n",
      "Test Dataset Count: 4754\n"
     ]
    }
   ],
   "source": [
    "# split the data into train and test\n",
    "train, test = dataset.randomSplit([0.7, 0.3], seed=100)\n",
    "\n",
    "# print the number of rows in train and test\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "# create the model\n",
    "nb = NaiveBayes(featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "# fit the model to the train data\n",
    "model = nb.fit(train)\n",
    "\n",
    "# predict the test data\n",
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/09 17:06:23 WARN DAGScheduler: Broadcasting large task binary with size 1199.2 KiB\n",
      "Test set accuracy = 0.6903660075725705\n",
      "22/12/09 17:06:24 WARN DAGScheduler: Broadcasting large task binary with size 1199.2 KiB\n",
      "Test set f1 score = 0.703776482543677\n"
     ]
    }
   ],
   "source": [
    "# find the test accuracy and f1 score\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# create the evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "# calculate the accuracy\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))\n",
    "\n",
    "# create the evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    ")\n",
    "\n",
    "# calculate the f1 score\n",
    "f1_score = evaluator.evaluate(predictions)\n",
    "print(\"Test set f1 score = \" + str(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'emotion', 'words', 'filtered', 'raw_features', 'features', 'label', 'rawPrediction', 'probability', 'prediction']\n"
     ]
    }
   ],
   "source": [
    "print(predictions.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/09 17:09:53 WARN DAGScheduler: Broadcasting large task binary with size 1183.7 KiB\n",
      "+-------+-----+----------+\n",
      "|emotion|label|prediction|\n",
      "+-------+-----+----------+\n",
      "|  anger|  2.0|       3.0|\n",
      "|  anger|  2.0|       0.0|\n",
      "|  anger|  2.0|       5.0|\n",
      "|    joy|  0.0|       3.0|\n",
      "|   fear|  3.0|       5.0|\n",
      "+-------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the emotion, label, prediction\n",
    "predictions.select(\"emotion\", \"label\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/09 17:11:01 WARN DAGScheduler: Broadcasting large task binary with size 1192.8 KiB\n",
      "+-------+-----+----------+\n",
      "|emotion|label|prediction|\n",
      "+-------+-----+----------+\n",
      "|   fear|  3.0|       3.0|\n",
      "|   fear|  3.0|       3.0|\n",
      "|sadness|  1.0|       1.0|\n",
      "|    joy|  0.0|       0.0|\n",
      "|  anger|  2.0|       2.0|\n",
      "+-------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show some predictions that are correct display only the emotion, label, prediction\n",
    "predictions.filter(predictions.label == predictions.prediction).select(\n",
    "    \"emotion\", \"label\", \"prediction\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/09 17:11:36 WARN DAGScheduler: Broadcasting large task binary with size 1194.8 KiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3282"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many predictions are correct and how many are wrong\n",
    "predictions.filter(predictions.label == predictions.prediction).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/09 17:11:40 WARN DAGScheduler: Broadcasting large task binary with size 1195.0 KiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1472"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many predictions are correct and how many are wrong\n",
    "predictions.filter(predictions.label != predictions.prediction).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4754"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what's the test set size\n",
    "test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/09 17:14:00 WARN DAGScheduler: Broadcasting large task binary with size 1206.4 KiB\n",
      "Test set accuracy = 0.8300378628523348\n",
      "22/12/09 17:14:00 WARN DAGScheduler: Broadcasting large task binary with size 1206.4 KiB\n",
      "Test set f1 score = 0.8317629919611031\n"
     ]
    }
   ],
   "source": [
    "# Let's try Logistic Regression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# create the model\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "# fit the model to the train data\n",
    "model = lr.fit(train)\n",
    "\n",
    "# predict the test data\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# create the evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "# calculate the accuracy\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "# print the accuracy\n",
    "print(\"Test set accuracy = \" + str(accuracy))\n",
    "\n",
    "# create the evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    ")\n",
    "\n",
    "# calculate the f1 score\n",
    "f1_score = evaluator.evaluate(predictions)\n",
    "\n",
    "# print the f1 score\n",
    "print(\"Test set f1 score = \" + str(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ai_ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4cb318141e9f7e3c14a32166c0dbdbe7778342fcbf7356c3b1f53fd17284d2ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
